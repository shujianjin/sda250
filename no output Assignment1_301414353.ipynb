{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "import pandas\n",
    "import csv\n",
    "import numpy\n",
    "from nltk.corpus import PlaintextCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The_Rat_Race='/Users/apple/Downloads/sda-binder-main/TheRatRace.txt'\n",
    "More_Science_From_an_Easy_Chair='/Users/apple/Downloads/sda-binder-main/More_Science_From_an_Easy_Chair.txt'\n",
    "The_sleeper_is_a_rebel=('/Users/apple/Downloads/sda-binder-main/The_sleeper_is_a_rebel .txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "corpus_root = \"/Users/apple/Downloads/sda-binder-main\"\n",
    "X11 = PlaintextCorpusReader(corpus_root, '.*', encoding = \"utf8\")\n",
    "corpus1 = X11.words('/Users/apple/Downloads/sda-binder-main/TheRatRace.txt')\n",
    "X22= PlaintextCorpusReader(corpus_root, '.*', encoding = \"utf8\")\n",
    "corpus2= X22.words('/Users/apple/Downloads/sda-binder-main/More_Science_From_an_Easy_Chair.txt')\n",
    "X33= PlaintextCorpusReader(corpus_root, '.*', encoding = \"utf8\")\n",
    "corpus3= X33.words('/Users/apple/Downloads/sda-binder-main/The_sleeper_is_a_rebel .txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexDiv(types, tokens):\n",
    "    diversity = types/tokens\n",
    "    print(\"Lexical Devisity=\", diversity)\n",
    "    return diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(set(corpus1))\n",
    "sorted(set(w.lower() for w in corpus1))\n",
    "Corpus1=sorted(set(w.lower() for w in corpus1 if w.isalpha()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(set(corpus2))\n",
    "sorted(set(w.lower() for w in corpus2))\n",
    "Corpus2=sorted(set(w.lower() for w in corpus2 if w.isalpha()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "#calculate tokens and types \n",
    "\n",
    "data = [corpus1, corpus2, corpus3]\n",
    "\n",
    "for file in data:\n",
    "    \n",
    "    print(\"types=\", len((set(file))))\n",
    "    print(\"tokens=\", len(file) )\n",
    "    sorted(set(file))\n",
    "    types= len(set(file))\n",
    "    tokens= len(file)\n",
    "    lexDiv(types, tokens)\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "#filter useless tokens\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "word_tokens1 = word_tokenize(X1)\n",
    "\n",
    "filtered_sentence = [w for w in word_tokens1 if not w in stop_words]\n",
    "\n",
    "filtered_sentence = []\n",
    "\n",
    "for w in word_tokens1:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "print(word_tokens1)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "word_tokens2 = word_tokenize(X2)\n",
    "\n",
    "filtered_sentence2 = [w for w in word_tokens2 if not w in stop_words]\n",
    "\n",
    "filtered_sentence2 = []\n",
    "\n",
    "for w in word_tokens2:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "print(word_tokens2)\n",
    "print(filtered_sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "word_tokens3 = word_tokenize(X3)\n",
    "\n",
    "filtered_sentence3 = [w for w in word_tokens3 if not w in stop_words]\n",
    "\n",
    "filtered_sentence3 = []\n",
    "\n",
    "for w in word_tokens3:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "print(word_tokens3)\n",
    "print(filtered_sentence3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "Fdist1 = FreqDist(word_tokens1)\n",
    "Fdist2 = FreqDist(word_tokens2)\n",
    "Fdist3 = FreqDist(word_tokens3)\n",
    "Fdist1.most_common(1000)\n",
    "Fdist2.most_common(1500)\n",
    "Fdist3.most_common(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(The_Rat_Race, \"r\", encoding=\"utf8\") as f:\n",
    "    X1=f.read()\n",
    "with open(More_Science_From_an_Easy_Chair, \"r\", encoding=\"utf8\") as f:\n",
    "    X2=f.read()\n",
    "with open(The_sleeper_is_a_rebel, \"r\", encoding=\"utf8\") as f:\n",
    "    X3=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_1 = nltk.sent_tokenize(X1)\n",
    "longest_sentences_1=max(sentences_1, key=len)\n",
    "number1=len(longest_sentences_1)\n",
    "print(max(sentences_1, key=len))\n",
    "\n",
    "sentences2 = nltk.sent_tokenize(X2)\n",
    "longest_sentences2=max(sentences2, key=len)\n",
    "number2=len(longest_sentences2)\n",
    "print(max(sentences2, key=len))\n",
    "\n",
    "sentences3 = nltk.sent_tokenize(X3)\n",
    "longest_sentences3=max(sentences3, key=len)\n",
    "number3=len(set(longest_sentences3))\n",
    "print(max(sentences3, key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "ps = PorterStemmer()\n",
    "Sentence1='You agree to indemnify and hold the Foundation, the trademark owner, any agent or employee of the Foundation, anyone providing copies of Project Gutenberg-tm electronic works in accordance with this agreement, and any volunteers associated with the production, promotion and distribution of Project Gutenberg-tm electronic works, harmless from all liability, costs and expenses, including legal fees, that arise directly or indirectly from any of the following which you do or cause to occur: (a) distribution of this or any Project Gutenberg-tm work, (b) alteration, modification, or additions or deletions to any Project Gutenberg-tm work, and (c) any Defect you cause.' \n",
    "TSentence1 = word_tokenize(Sentence1)\n",
    "print(len(TSentence1))\n",
    "for w in TSentence1:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sentence2=\"Ancient Painted Glass in England; Archæology and False Antiquities; The Bells of England; The Brasses of England; Celtic Art in Pagan and Christian Times; Churchwardens' Accounts; The Domesday Inquest; The Castles and Walled Towns of England; English Church Furniture; English Costume, from Prehistoric Times to the End of the Eighteenth Century; English Monastic Life; English Seals; Folk-Lore as an Historical Science; The Gilds and Companies of London; The Hermits and Anchorites of England; The Manor and Manorial Records; The Mediæval Hospitals of England; Old English Instruments of Music; Old English Libraries; Old Service Books of the English Church; Parish Life in Mediæval England; The Parish Registers of England; Remains of the Prehistoric Age in England; The Roman Era in Britain; Romano-British Buildings and Earthworks; The Royal Forests of England; The Schools of Mediæval England; Shrines of British Saints.\"\n",
    "TSentence2=word_tokenize(Sentence2)\n",
    "print(len(TSentence2))\n",
    "for w in TSentence2:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sentence3=\" He listened to the hunchback's resigned, introspective, and rather sad song: Never shall I strain my dull eyes more For hidden meaning; I shall be as one Who in his dim perception of the skies Sees but the certain glory of the sun And does not care to question or devise An answer to his own oblivion.\"\n",
    "TSentence3=word_tokenize(Sentence3)\n",
    "print(len(TSentence3))\n",
    "for w in TSentence3:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "def getcollocation1():\n",
    "       wordlists=PlaintextCorpusReader(corpus_root,'.*')\n",
    "       x=nltk.Text(wordlists.words('/Users/apple/Downloads/sda-binder-main/TheRatRace.txt'))\n",
    "       print  (x.collocations(5))\n",
    "getcollocation1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "def getcollocation2():\n",
    "       wordlists=PlaintextCorpusReader(corpus_root,'.*')\n",
    "       x=nltk.Text(wordlists.words('/Users/apple/Downloads/sda-binder-main/More_Science_From_an_Easy_Chair.txt'))\n",
    "       print  (x.collocations(5))\n",
    "getcollocation2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "def getcollocation3():\n",
    "       wordlists=PlaintextCorpusReader(corpus_root,'.*')\n",
    "       x=nltk.Text(wordlists.words('/Users/apple/Downloads/sda-binder-main/The_sleeper_is_a_rebel .txt'))\n",
    "       print  (x.collocations(5))\n",
    "getcollocation3()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1 (v3.11.1:a7a450f84a, Dec  6 2022, 15:24:06) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
